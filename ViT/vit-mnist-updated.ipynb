{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Libraries","metadata":{"id":"z1OE4T9lIy7q"}},{"cell_type":"code","source":"pip install pytorch_msssim","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1NEXRXTc7HVi","outputId":"7cfd6195-aaef-439b-f584-9431add4c143","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:43:55.708084Z","iopub.execute_input":"2025-10-26T06:43:55.708795Z","iopub.status.idle":"2025-10-26T06:43:59.030725Z","shell.execute_reply.started":"2025-10-26T06:43:55.708754Z","shell.execute_reply":"2025-10-26T06:43:59.029856Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytorch_msssim in /usr/local/lib/python3.11/dist-packages (1.0.0)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from pytorch_msssim) (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (2025.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->pytorch_msssim) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->pytorch_msssim) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->pytorch_msssim) (3.0.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.utils as vutils\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport numpy as np\nfrom pytorch_msssim import ssim","metadata":{"id":"k1dipFHvHZJE","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:43:59.032419Z","iopub.execute_input":"2025-10-26T06:43:59.032740Z","iopub.status.idle":"2025-10-26T06:44:02.297655Z","shell.execute_reply.started":"2025-10-26T06:43:59.032716Z","shell.execute_reply":"2025-10-26T06:44:02.296957Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Define hyperparameters","metadata":{"id":"JXS32eiSI6G6"}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\n# Training classifier params\nCLS_EPOCHS = 5\nCLS_BATCH = 128\nCLS_LR = 0.1\n\n# Generator / inversion params\nGEN_EPOCHS = 10\nGEN_BATCH = 64\nlatent_dim = 100\ncond_dim = 10\nF_ch = 64                 # channels at 10x10 stage\nalpha, beta, gamma, delta = 100.0, 100.0, 1000.0, 1000.0\n\n# Other\nNUM_CLASSES = 10\nSAMPLES_PER_CLASS = 20    # for evaluation\nSAVE_MODEL_PATH = \"vit_cifar10.pth\"\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)","metadata":{"id":"LpLKxDARHWG3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"23b4da17-80d7-4718-e920-fff48a4c34d5","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:02.298504Z","iopub.execute_input":"2025-10-26T06:44:02.299250Z","iopub.status.idle":"2025-10-26T06:44:02.364737Z","shell.execute_reply.started":"2025-10-26T06:44:02.299218Z","shell.execute_reply":"2025-10-26T06:44:02.363969Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Data loaders","metadata":{"id":"hfJ_B3dFI_Ab"}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize(32),     # Resize MNIST to 32x32 so generator architecture can remain same\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\ntrainset = torchvision.datasets.MNIST(root='./data', train=True,\n                                      download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n                                          shuffle=True)\n\ntestset = torchvision.datasets.MNIST(root='./data', train=False,\n                                     download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=100,\n                                         shuffle=False)\n","metadata":{"id":"AdCnohUMHcn3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d55819d8-2de4-4916-fe57-ee81135e6150","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:02.366238Z","iopub.execute_input":"2025-10-26T06:44:02.366502Z","iopub.status.idle":"2025-10-26T06:44:04.018778Z","shell.execute_reply.started":"2025-10-26T06:44:02.366484Z","shell.execute_reply":"2025-10-26T06:44:04.017986Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"len(trainloader)","metadata":{"id":"tASpZJk98sri","outputId":"578a54e2-19aa-4b0a-bda3-d2783201ae26","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:04.019608Z","iopub.execute_input":"2025-10-26T06:44:04.019816Z","iopub.status.idle":"2025-10-26T06:44:04.025774Z","shell.execute_reply.started":"2025-10-26T06:44:04.019798Z","shell.execute_reply":"2025-10-26T06:44:04.025024Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"391"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Classifier (ViT)","metadata":{"id":"XOPSppHQJC1G"}},{"cell_type":"code","source":"class ViTClassifier(nn.Module):\n    def __init__(self, num_classes=10, pretrained=True):\n        super().__init__()\n        # Load pretrained Vision Transformer (ImageNet)\n        weights = models.ViT_B_16_Weights.IMAGENET1K_V1 if pretrained else None\n        self.vit = models.vit_b_16(weights=weights)\n\n        # Replace classification head\n        in_features = self.vit.heads.head.in_features\n        self.vit.heads.head = nn.Identity() # Replace with Identity to get features before the head\n\n        # Optional: adapt for 32x32 (resize patch embedding)\n        self.resize = nn.Upsample(size=(224, 224), mode=\"bilinear\", align_corners=False)\n        self.classifier_head = nn.Linear(in_features, num_classes) # New classification head\n\n    def forward(self, x, return_features=False):\n        # Upsample small CIFAR-10 image to 224x224 expected by ViT\n        x = self.resize(x)\n\n        # Extract penultimate features\n        features = self.vit(x) # Get features after the identity head\n        logits = self.classifier_head(features) # Pass features through the new head\n\n\n        if return_features:\n            return logits, features\n        return logits\n\n\n\n# Instantiate\nclassifier = ViTClassifier(num_classes=NUM_CLASSES).to(device)","metadata":{"id":"K1zkFLxtHjj8","colab":{"base_uri":"https://localhost:8080/"},"outputId":"24c11294-2af2-4ad3-d159-db532b090b6f","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:04.026884Z","iopub.execute_input":"2025-10-26T06:44:04.027127Z","iopub.status.idle":"2025-10-26T06:44:05.542938Z","shell.execute_reply.started":"2025-10-26T06:44:04.027104Z","shell.execute_reply":"2025-10-26T06:44:05.542319Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Classifier training utilities\n","metadata":{"id":"cHQRZx1YJHjY"}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(classifier.parameters(), lr=3e-5)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20)\n    \ndef train_classifier_one_epoch(epoch):\n    classifier.train()\n    running_loss, correct, total = 0.0, 0, 0\n    start = time.time()\n    for inputs, targets in trainloader:\n        inputs, targets = inputs.to(device), (targets).to(device)\n        optimizer.zero_grad()\n        outputs = classifier(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item() * targets.size(0)\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n    train_loss = running_loss / total\n    train_acc = 100. * correct / total\n    print(f\"[CLS] Epoch {epoch} Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% Time: {time.time()-start:.1f}s\")\n    return train_loss, train_acc\n\ndef test_classifier(epoch):\n    classifier.eval()\n    running_loss, correct, total = 0.0, 0, 0\n    with torch.no_grad():\n        for inputs, targets in testloader:\n            inputs, targets = inputs.to(device), (targets).to(device)\n            outputs = classifier(inputs)\n            loss = criterion(outputs, targets)\n            running_loss += loss.item() * targets.size(0)\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n    test_loss = running_loss / total\n    acc = 100. * correct / total\n    print(f\"[CLS] Epoch {epoch} Test Loss: {test_loss:.4f} Acc: {acc:.2f}%\")\n    return test_loss, acc","metadata":{"id":"4ULnu-NSHoQU","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:05.543760Z","iopub.execute_input":"2025-10-26T06:44:05.543975Z","iopub.status.idle":"2025-10-26T06:44:05.553121Z","shell.execute_reply.started":"2025-10-26T06:44:05.543956Z","shell.execute_reply":"2025-10-26T06:44:05.552360Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Train classifier\n","metadata":{"id":"zpPDTE7hJKpp"}},{"cell_type":"code","source":"def train_classifier(epochs=CLS_EPOCHS):\n    best_acc = 0.0\n    train_losses, train_accs = [], []\n    test_losses, test_accs = [], []\n    for ep in range(1, epochs+1):\n        tr_loss, tr_acc = train_classifier_one_epoch(ep)\n        te_loss, te_acc = test_classifier(ep)\n        scheduler.step()\n        train_losses.append(tr_loss); train_accs.append(tr_acc)\n        test_losses.append(te_loss); test_accs.append(te_acc)\n        if te_acc > best_acc:\n            best_acc = te_acc\n            torch.save(classifier.state_dict(), SAVE_MODEL_PATH)\n            print(f\"[CLS] Saved best model (acc={best_acc:.2f}%)\")\n    # plot training curves\n    plt.figure(figsize=(10,4))\n    plt.subplot(1,2,1)\n    plt.plot(range(1,len(train_accs)+1), train_accs, label='Train Acc')\n    plt.plot(range(1,len(test_accs)+1), test_accs, label='Test Acc')\n    plt.title(\"Accuracy\")\n    plt.legend()\n    plt.subplot(1,2,2)\n    plt.plot(range(1,len(train_losses)+1), train_losses, label='Train Loss')\n    plt.plot(range(1,len(test_losses)+1), test_losses, label='Test Loss')\n    plt.title(\"Loss\")\n    plt.legend()\n    plt.show()\n\n# Only train classifier if model file not present\nif not os.path.isfile(SAVE_MODEL_PATH):\n    print(\"Training classifier from scratch...\")\n    train_classifier(epochs=CLS_EPOCHS)\nelse:\n    print(\"Found saved classifier. Loading...\")\n    classifier.load_state_dict(torch.load(SAVE_MODEL_PATH, map_location=device))\n\n# Freeze classifier (but allow autograd to flow into generator through the classifier's ops)\nfor p in classifier.parameters():\n    p.requires_grad = False\nclassifier.eval()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"2wO7CDloH4TG","outputId":"4bee7ccd-7f51-4594-cee1-b2c7efb69372","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:05.553751Z","iopub.execute_input":"2025-10-26T06:44:05.553942Z","iopub.status.idle":"2025-10-26T06:44:07.815755Z","shell.execute_reply.started":"2025-10-26T06:44:05.553926Z","shell.execute_reply":"2025-10-26T06:44:07.813946Z"}},"outputs":[{"name":"stdout","text":"Training classifier from scratch...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_151/3421742198.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSAVE_MODEL_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training classifier from scratch...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mtrain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLS_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found saved classifier. Loading...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_151/3421742198.py\u001b[0m in \u001b[0;36mtrain_classifier\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_classifier_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mte_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_151/1966403898.py\u001b[0m in \u001b[0;36mtrain_classifier_one_epoch\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_151/816343681.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_features)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Extract penultimate features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get features after the identity head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pass features through the new head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_class_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# Classifier \"token\" as used by standard language architectures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/vision_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1371\u001b[0m             )\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1374\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6228\u001b[0m             \u001b[0min_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6229\u001b[0m         ), \"use_separate_proj_weight is False but in_proj_weight is None\"\n\u001b[0;32m-> 6230\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6232\u001b[0m         assert (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   5619\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5620\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5621\u001b[0;31m                 \u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5622\u001b[0m             )\n\u001b[1;32m   5623\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 74.12 MiB is free. Process 8534 has 14.67 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 24.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 222.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 74.12 MiB is free. Process 8534 has 14.67 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 24.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":8},{"cell_type":"markdown","source":"# Conditioning utils & precompute hot matrices\n","metadata":{"id":"dUJILmBDJSBT"}},{"cell_type":"code","source":"# We will pass the **softmaxed** conditioning vector INTO the generator (so it's a distribution).\ndef sample_vector_condition(batch, num_classes, device):\n    v = torch.randn(batch, num_classes, device=device)\n    P = F.softmax(v, dim=1)      # conditioning distribution P (batch, num_classes)\n    labels = P.argmax(dim=1)     # integer labels used in CE target\n    return P, P, labels         # return vector_cond (P), P_target, labels\n\n# Precompute 'hot' matrices for all classes and reuse\n_precomputed_hot = torch.zeros(NUM_CLASSES, 1, NUM_CLASSES, NUM_CLASSES, device=device)\nfor k in range(NUM_CLASSES):\n    mat = torch.zeros(NUM_CLASSES, NUM_CLASSES, device=device)\n    mat[k, :] = 1.0\n    mat[:, k] = 1.0\n    _precomputed_hot[k, 0] = mat\n\ndef build_hot_matrix(labels, N=NUM_CLASSES, device=device):\n    # labels: (batch,) ints on device\n    # returns (batch,1,10,10) by creating appropriate matrices\n    batch_size = labels.size(0)\n    hot_mats = torch.zeros(batch_size, 1, 10, 10, device=device)\n\n    for i, label in enumerate(labels):\n        # Create a 10x10 matrix with the label information\n        # You can experiment with different patterns here\n        mat = torch.zeros(10, 10, device=device)\n\n        # Simple pattern: set rows and columns corresponding to label\n        mat[label, :] = 1.0\n        mat[:, label] = 1.0\n\n        hot_mats[i, 0] = mat\n\n    return hot_mats","metadata":{"id":"XcmB2a44H4rD","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:07.816377Z","iopub.status.idle":"2025-10-26T06:44:07.816619Z","shell.execute_reply.started":"2025-10-26T06:44:07.816490Z","shell.execute_reply":"2025-10-26T06:44:07.816501Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generator\n\n","metadata":{"id":"5mERgDzRJWp2"}},{"cell_type":"code","source":"# Fix the generator architecture first\nclass Generator(nn.Module):\n    def __init__(self, latent_dim=latent_dim, cond_dim=cond_dim, F=F_ch, out_ch=1):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.cond_dim = cond_dim\n\n        # Initial projection to 4x4\n        self.convT1 = nn.ConvTranspose2d(latent_dim + cond_dim, F * 8, kernel_size=4, stride=1, padding=0)  # 1x1 -> 4x4\n        self.bn1 = nn.BatchNorm2d(F * 8)\n\n        # 4x4 -> 8x8\n        self.convT2 = nn.ConvTranspose2d(F * 8 + 1, F * 4, kernel_size=4, stride=2, padding=1)  # +1 for hot channel\n        self.bn2 = nn.BatchNorm2d(F * 4)\n\n        # 8x8 -> 16x16\n        self.convT3 = nn.ConvTranspose2d(F * 4, F * 2, kernel_size=4, stride=2, padding=1)\n        self.bn3 = nn.BatchNorm2d(F * 2)\n\n        # 16x16 -> 32x32\n        self.convT4 = nn.ConvTranspose2d(F * 2, F, kernel_size=4, stride=2, padding=1)\n        self.bn4 = nn.BatchNorm2d(F)\n\n        # Final convolution to get 3 channels\n        self.final_conv = nn.Conv2d(F, out_ch, kernel_size=3, stride=1, padding=1)\n        self.dropout = nn.Dropout2d(0.2)  # Reduced dropout\n\n    def forward(self, z, vec_cond, hot_mat):\n        # z: (B, latent), vec_cond: (B, cond_dim), hot_mat: (B,1,10,10)\n        x = torch.cat([z, vec_cond], dim=1)           # (B, latent+cond)\n        x = x.view(x.size(0), x.size(1), 1, 1)        # (B, latent+cond,1,1)\n\n        # 1x1 -> 4x4\n        x = F.leaky_relu(self.bn1(self.convT1(x)), 0.2)\n\n        # Resize hot_mat to match current feature size (4x4)\n        hot_resized = F.interpolate(hot_mat, size=(4, 4), mode='bilinear', align_corners=False)\n        x = torch.cat([x, hot_resized], dim=1)\n\n        # 4x4 -> 8x8\n        x = F.leaky_relu(self.bn2(self.convT2(x)), 0.2)\n        x = self.dropout(x)\n\n        # 8x8 -> 16x16\n        x = F.leaky_relu(self.bn3(self.convT3(x)), 0.2)\n\n        # 16x16 -> 32x32\n        x = F.leaky_relu(self.bn4(self.convT4(x)), 0.2)\n\n        img = torch.tanh(self.final_conv(x))  # range [-1,1]\n        return img","metadata":{"id":"jBa4MifFH-vu","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:07.817936Z","iopub.status.idle":"2025-10-26T06:44:07.818245Z","shell.execute_reply.started":"2025-10-26T06:44:07.818083Z","shell.execute_reply":"2025-10-26T06:44:07.818099Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss utilities","metadata":{"id":"6CY0iDADJdoS"}},{"cell_type":"code","source":"def kl_divergence_targetP(logits, P_target):\n    # D_KL(P_target || Q_pred)\n    # logits: (B,C); P_target: (B,C)\n    log_softmax = F.log_softmax(logits, dim=1)\n    kl_div = F.kl_div(log_softmax, P_target, reduction='batchmean', log_target=False)\n    return kl_div\n\ndef cosine_similarity_loss(features):\n    # features: (B, D)\n    eps = 1e-8\n    f = features / (features.norm(dim=1, keepdim=True) + eps)  # normalize\n    sim = f @ f.T  # (B,B)\n    B = features.size(0)\n    off_diag_sum = sim.sum() - B  # subtract diagonal ones\n    denom = B * (B - 1)\n    return off_diag_sum / (denom + 1e-12)\n\ndef orthogonality_loss(features):\n    # make normalized features orthogonal (Gram approx I)\n    eps = 1e-8\n    f = features / (features.norm(dim=1, keepdim=True) + eps)\n    G = f @ f.T\n    B = features.size(0)\n    I = torch.eye(B, device=features.device)\n    return ((G - I)**2).mean()\ndef ssim_loss(imgs, ref_imgs):\n    \"\"\"\n    Structural Similarity (SSIM) loss between generated and reference images.\n    Both tensors in range [-1, 1].\n    Returns (1 - SSIM) averaged over the batch.\n    \"\"\"\n    return 1 - ssim(imgs, ref_imgs, data_range=2.0, size_average=True)\n# Improved loss function with better balancing\ndef compute_inversion_loss(\n    logits, features, P_target, labels,\n    imgs=None, ref_imgs=None,\n    alpha=1.0, beta=1.0, gamma=0.01, delta=0.01, eta=0.5\n):\n    \"\"\"\n    Total inversion loss = α * KL + β * CE + γ * CosSim + δ * Orthogonality + η * (1 - SSIM)\n    \"\"\"\n\n    # Core inversion losses\n    L_kl = kl_divergence_targetP(logits, P_target)\n    L_ce = F.cross_entropy(logits, labels)\n    L_cos = cosine_similarity_loss(features)\n    L_ortho = orthogonality_loss(features)\n\n    # SSIM Loss (optional, requires reference images)\n    if imgs is not None and ref_imgs is not None:\n        L_ssim = ssim_loss(imgs, ref_imgs)\n    else:\n        L_ssim = torch.tensor(0.0, device=logits.device)\n\n    total_loss = (\n        alpha * L_kl +\n        beta * L_ce +\n        gamma * L_cos +\n        delta * L_ortho +\n        eta * L_ssim\n    )\n\n    return {\n        \"L_kl\": L_kl,\n        \"L_ce\": L_ce,\n        \"L_cos\": L_cos,\n        \"L_ortho\": L_ortho,\n        \"L_ssim\": L_ssim,\n        \"total\": total_loss\n    }\n\n","metadata":{"id":"Uf0y_05FIBHN","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:07.819613Z","iopub.status.idle":"2025-10-26T06:44:07.820105Z","shell.execute_reply.started":"2025-10-26T06:44:07.819974Z","shell.execute_reply":"2025-10-26T06:44:07.819987Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inversion training\n","metadata":{"id":"QL9aqTL3JgAr"}},{"cell_type":"code","source":"alpha, beta, gamma, delta, eta = 100.0, 200.0, 1000.0, 1000.0, 1000.0  # tuned weights\ngen = Generator().to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr=1e-4, betas=(0.5, 0.999))\n\ndef train_generator(epochs=GEN_EPOCHS, batch_size=GEN_BATCH):\n    gen.train()\n\n    for ep in range(1, epochs + 1):\n        epoch_loss = epoch_Lkl = epoch_Lce = epoch_Lcos = epoch_Lortho = epoch_Lssim = 0.0\n        t0 = time.time()\n        steps = 200\n\n        for step in range(steps):\n            # Sample conditioning\n            vec_cond, P_target, labels = sample_vector_condition(batch_size, NUM_CLASSES, device)\n            z = torch.randn(batch_size, latent_dim, device=device)\n            hot = build_hot_matrix(labels, N=NUM_CLASSES, device=device)\n\n            # Generate images\n            imgs = gen(z, vec_cond, hot)\n\n            # Create structural reference (slightly perturbed copy)\n            ref_imgs = imgs + 0.02 * torch.randn_like(imgs)\n\n            # Forward through frozen classifier\n            logits, features = classifier(imgs, return_features=True)\n\n            # Compute combined inversion + SSIM loss\n            loss_dict = compute_inversion_loss(\n                logits, features, P_target, labels,\n                imgs=imgs, ref_imgs=ref_imgs,\n                alpha=alpha, beta=beta, gamma=gamma, delta=delta, eta=eta\n            )\n\n            total_loss = loss_dict[\"total\"] # Rename the variable to avoid conflict\n\n            gen_opt.zero_grad()\n            total_loss.backward() # Use the renamed variable\n            torch.nn.utils.clip_grad_norm_(gen.parameters(), max_norm=1.0)\n            gen_opt.step()\n\n            # Accumulate stats\n            epoch_loss += total_loss.item()\n            epoch_Lkl += loss_dict[\"L_kl\"].item()\n            epoch_Lce += loss_dict[\"L_ce\"].item()\n            epoch_Lcos += loss_dict[\"L_cos\"].item()\n            epoch_Lortho += loss_dict[\"L_ortho\"].item()\n            epoch_Lssim += loss_dict[\"L_ssim\"].item()\n\n\n        # Epoch summary\n        steps = float(steps)\n        print(f\"[GEN] Ep {ep}/{epochs} | Loss {epoch_loss/steps:.4f} | \"\n              f\"KL {epoch_Lkl/steps:.4f} | CE {epoch_Lce/steps:.4f} | \"\n              f\"COS {epoch_Lcos/steps:.4f} | ORT {epoch_Lortho/steps:.4f} | \"\n              f\"SSIM {epoch_Lssim/steps:.4f} | time {time.time()-t0:.1f}s\")\n\n        # Evaluate & visualize every few epochs\n        if ep % 10 == 0:\n            acc = inversion_accuracy(gen, classifier)\n            print(f\"[GEN] Inversion Accuracy: {acc:.2f}%\")\n            show_generated_images(gen, classifier, samples_per_class=4)\n            torch.save(gen.state_dict(), f\"gen_epoch{ep}.pth\")","metadata":{"id":"RHej58VxIFdJ","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:07.820841Z","iopub.status.idle":"2025-10-26T06:44:07.821177Z","shell.execute_reply.started":"2025-10-26T06:44:07.821046Z","shell.execute_reply":"2025-10-26T06:44:07.821060Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation\n","metadata":{"id":"dSO03BT8Jl4b"}},{"cell_type":"code","source":"def inversion_accuracy(\n    gen,\n    classifier,\n    num_classes=NUM_CLASSES,\n    n_per_class=100,\n    latent_dim=latent_dim,\n    batch_size=32\n):\n    \"\"\"\n    Inversion Accuracy refers to the percentage of images generated with desired labels same as the output labels from the classifier.\n    \"\"\"\n    gen.eval()\n    classifier.eval()\n\n    correct, total = 0, 0\n\n    with torch.no_grad():\n        for cls in range(num_classes):\n            for start in range(0, n_per_class, batch_size):\n                bsz = min(batch_size, n_per_class - start)\n                z = torch.randn(bsz, latent_dim, device=device)\n                c = torch.zeros(bsz, num_classes, device=device)\n                c[:, cls] = 1.0\n                hot = build_hot_matrix(torch.full((bsz,), cls, device=device), N=num_classes, device=device)\n\n                imgs = gen(z, c, hot)\n                logits, _ = classifier(imgs, return_features=True)\n                preds = logits.argmax(dim=1)\n\n                correct += (preds == cls).sum().item()\n                total += bsz\n\n    return 100.0 * correct / total\n\n\ndef show_generated_images(\n    gen,\n    classifier,\n    latent_dim=latent_dim,\n    num_classes=NUM_CLASSES,\n    samples_per_class=8\n):\n    gen.eval()\n    classifier.eval()\n\n    with torch.no_grad():\n        fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(samples_per_class, num_classes))\n\n        for cls in range(num_classes):\n            # 1. Sample Latent + Conditioning\n            z = torch.randn(samples_per_class, latent_dim, device=device)\n            c = torch.zeros(samples_per_class, num_classes, device=device)\n            c[:, cls] = 1.0  # one-hot class conditioning\n            hot = build_hot_matrix(torch.full((samples_per_class,), cls, device=device),\n                                   N=num_classes, device=device)\n\n            # 2. Generate Images\n            imgs = gen(z, c, hot)\n\n            # 3. Classifier Prediction (batch-based → efficient)\n            logits, _ = classifier(imgs, return_features=True)\n            preds = logits.argmax(dim=1)\n\n            # 4. Convert to displayable format\n            imgs = (imgs + 1.0) / 2.0\n            imgs = imgs.clamp(0, 1)\n\n            for i in range(samples_per_class):\n                ax = axes[cls, i] if num_classes > 1 else axes[i]\n                img = imgs[i].cpu().squeeze().numpy()\n                plt.imshow(img, cmap='gray')\n\n\n                ax.imshow(img)\n                ax.axis(\"off\")\n\n                true_label = cls\n                pred_label = preds[i].item()\n                color = \"green\" if true_label == pred_label else \"red\"\n\n                ax.set_title(f\"T:{true_label} | P:{pred_label}\", fontsize=8, color=color)\n\n        plt.tight_layout()\n        plt.show()\n\n\ndef eval_tsne_grid(\n    gen,\n    classifier,\n    samples_per_class=100,\n    num_classes=NUM_CLASSES,\n    latent_dim=latent_dim,\n    seed=42\n):\n    gen.eval()\n    classifier.eval()\n    all_features, all_labels, all_preds = [], [], []\n\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n\n    with torch.no_grad():\n        for label in range(num_classes):\n            labels = torch.full((samples_per_class,), label, dtype=torch.long, device=device)\n            v_raw = F.softmax(torch.randn(samples_per_class, num_classes, device=device), dim=1)\n            hot_mat = build_hot_matrix(labels, N=num_classes, device=device)\n            z = torch.randn(samples_per_class, latent_dim, device=device)\n\n            imgs = gen(z, v_raw, hot_mat)\n            logits, feats = classifier(imgs, return_features=True)\n            preds = logits.argmax(dim=1)\n\n            all_features.append(feats.cpu())\n            all_labels.append(labels.cpu())\n            all_preds.append(preds.cpu())\n\n    # Concatenate\n    all_features = torch.cat(all_features, dim=0).numpy()\n    all_labels = torch.cat(all_labels, dim=0).numpy()\n    all_preds = torch.cat(all_preds, dim=0).numpy()\n\n    acc = np.mean(all_labels == all_preds) * 100\n    print(f\"[t-SNE] Inversion Accuracy: {acc:.2f}%\")\n\n    # Dimensionality reduction\n    pca = PCA(n_components=min(50, all_features.shape[1]))\n    features_pca = pca.fit_transform(all_features)\n\n    tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=seed)\n    features_2d = tsne.fit_transform(features_pca)\n    plt.figure(figsize=(8, 6))\n    for label in range(num_classes):\n        idx = all_labels == label\n        plt.scatter(features_2d[idx, 0], features_2d[idx, 1], label=f\"Class {label}\", alpha=0.7, s=12)\n    plt.legend()\n    plt.title(\"t-SNE of Generator Feature Embeddings\")\n    plt.xlabel(\"Dim 1\")\n    plt.ylabel(\"Dim 2\")\n    plt.show()","metadata":{"id":"lllGPogQIRRM","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:07.822126Z","iopub.status.idle":"2025-10-26T06:44:07.822500Z","shell.execute_reply.started":"2025-10-26T06:44:07.822308Z","shell.execute_reply":"2025-10-26T06:44:07.822325Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Run generator training\n","metadata":{"id":"ve3r-J6kJrX3"}},{"cell_type":"code","source":"print(\"Starting generator training...\")\ntrain_generator(epochs=10, batch_size=64)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"sQUNpLwhIRmZ","outputId":"a9e287b9-9086-40cc-bd1d-afb1f6c2de46","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:07.823579Z","iopub.status.idle":"2025-10-26T06:44:07.823893Z","shell.execute_reply.started":"2025-10-26T06:44:07.823737Z","shell.execute_reply":"2025-10-26T06:44:07.823753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final t-SNE visualization\nprint(\"Running t-SNE evaluation...\")\neval_tsne_grid(gen, classifier, samples_per_class=SAMPLES_PER_CLASS)","metadata":{"id":"7kejZaKWJdRF","colab":{"base_uri":"https://localhost:8080/","height":637},"outputId":"634693ff-3a79-4fc3-f139-d92a7d31552e","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:07.824844Z","iopub.status.idle":"2025-10-26T06:44:07.825062Z","shell.execute_reply.started":"2025-10-26T06:44:07.824964Z","shell.execute_reply":"2025-10-26T06:44:07.824974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Final grid of generated images\nprint(\"Generating final image grid...\")\nshow_generated_images(gen, classifier, samples_per_class=8)","metadata":{"id":"fKaEjgALJf5P","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"72b92c2c-7259-40bc-9d3a-356395da378b","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:07.825766Z","iopub.status.idle":"2025-10-26T06:44:07.825970Z","shell.execute_reply.started":"2025-10-26T06:44:07.825876Z","shell.execute_reply":"2025-10-26T06:44:07.825885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_acc = inversion_accuracy(gen, classifier, n_per_class=100)\nprint(f\"FINAL INVERSION ACCURACY: {final_acc:.2f}%\")","metadata":{"id":"PaBmpyJ_Nq1L","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8eabbde6-39eb-4683-eb4c-b72ff8dadcc0","trusted":true,"execution":{"iopub.status.busy":"2025-10-26T06:44:07.827384Z","iopub.status.idle":"2025-10-26T06:44:07.827683Z","shell.execute_reply.started":"2025-10-26T06:44:07.827506Z","shell.execute_reply":"2025-10-26T06:44:07.827522Z"}},"outputs":[],"execution_count":null}]}